import pandas as pd
import json
import os
import re
import time
from typing import List, Literal
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from dotenv import load_dotenv

import sys
import io

# ì‹œìŠ¤í…œ ì…ì¶œë ¥ì„ UTF-8ë¡œ ê°•ì œ ê³ ì • (ëª¨ë“  íŒŒì¼ ê³µí†µ ì ìš© ê¶Œì¥)
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
if sys.stderr.encoding != 'utf-8':
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

load_dotenv()


OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
URI = os.getenv("NEO4J_URI")
USER = os.getenv("NEO4J_USER")
PWD = os.getenv("NEO4J_PASSWORD")
AUTH = (USER, PWD)

# --- [2. ë°ì´í„° êµ¬ì¡° ì •ì˜] ---
class Node(BaseModel):
    id: str = Field(description="ê³ ìœ  ID")
    label: str = Field(description="ë…¸ë“œ íƒ€ì… (Company, Report, Rating, Pillar, Theme, Content)")
    name: str = Field(description="ì—”í‹°í‹°ì˜ ì‹¤ì œ ì´ë¦„")

class Relationship(BaseModel):
    type: Literal["HAS_REPORT", "HAS_RATING", "HAS_CONTENT", "HAS_CATEGORY", "HAS_THEME"]
    start_node_id: str
    end_node_id: str

class GraphResponse(BaseModel):
    nodes: List[Node]
    relationships: List[Relationship]

# --- [3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def normalize_id(label, name):
    if pd.isna(name): return f"{label}_unknown"
    clean_name = re.sub(r'[^\w\s]', '', str(name)).strip().lower().replace(' ', '_')
    return f"{label}_{clean_name}"

def merge_graphs(raw_data_list):
    combined_nodes = {}
    combined_relationships = []
    
    for chunk in raw_data_list:
        if not chunk: continue
        id_map = {}
        for node in chunk.get('nodes', []):
            old_id = node['id']
            new_id = normalize_id(node['label'], node['name'])
            id_map[old_id] = new_id
            combined_nodes[new_id] = {"id": new_id, "label": node['label'], "name": node['name']}

        for rel in chunk.get('relationships', []):
            start_id = id_map.get(rel['start_node_id'], rel['start_node_id'])
            end_id = id_map.get(rel['end_node_id'], rel['end_node_id'])
            combined_relationships.append({"type": rel['type'], "start_node_id": start_id, "end_node_id": end_id})

    unique_rels = []
    seen_rels = set()
    for r in combined_relationships:
        rel_tuple = (r['type'], r['start_node_id'], r['end_node_id'])
        if rel_tuple not in seen_rels:
            seen_rels.add(rel_tuple)
            unique_rels.append(r)
            
    return {"nodes": list(combined_nodes.values()), "relationships": unique_rels}

# --- [4. ë©”ì¸ ì‹¤í–‰ ë¡œì§] ---
if __name__ == "__main__":
    FILE_PATH = os.getenv("DATA_FILE_PATH", "data/esg_database.csv") 
    OUTPUT_DIR = "output"
    CHECKPOINT_FILE = os.path.join(OUTPUT_DIR, "checkpoint_graphs.json")
    FINAL_OUTPUT_FILE = os.path.join(OUTPUT_DIR, "final_merged_graph_full.json")
    
    BATCH_SIZE = 5

    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)

    # api_key=OPENAI_API_KEY ë¡œ ìˆ˜ì • ì™„ë£Œ
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=OPENAI_API_KEY)
    structured_llm = llm.with_structured_output(GraphResponse)

    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an ESG Knowledge Graph engineer. 
        Extract ALL entities and relationships from the provided CSV data.
        Multiple rows are provided. Process them all into one connected graph.
        Schema:
        - (Company)-[:HAS_REPORT]->(Report)
        - (Report)-[:HAS_RATING]->(Rating)
        - (Report)-[:HAS_CONTENT]->(Content)
        - (Report)-[:HAS_CATEGORY]->(Pillar)-[:HAS_THEME]->(Theme)"""),
        ("human", "Input CSV Data (Multiple Rows):\n{input_text}")
    ])

    # ë°ì´í„° ë¡œë“œ
    try:
        df = pd.read_csv(FILE_PATH, encoding="utf-8-sig")
    except Exception:
        try:
            df = pd.read_csv(FILE_PATH, encoding="cp949")
        except Exception as e:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            df = pd.DataFrame()

    if not df.empty:
        all_raw_graphs = []
        processed_count = 0

        if os.path.exists(CHECKPOINT_FILE):
            with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
                all_raw_graphs = json.load(f)
            processed_count = len(all_raw_graphs) * BATCH_SIZE 
            print(f"ğŸ”„ ì´ì „ ê¸°ë¡ ë°œê²¬. ì•½ {processed_count}í–‰ ì´í›„ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")

        total_rows = len(df)
        chain = prompt | structured_llm

        for i in range(processed_count, total_rows, BATCH_SIZE):
            batch_df = df.iloc[i : i + BATCH_SIZE]
            print(f"[{min(i+BATCH_SIZE, total_rows)}/{total_rows}] ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")

            batch_text = ""
            for idx, row in batch_df.iterrows():
                row_text = " / ".join([f"{col}: {val}" for col, val in row.items() if pd.notna(val)])
                batch_text += f"[Row {idx+1}]\n{row_text}\n\n"

            try:
                response = chain.invoke({"input_text": batch_text})
                all_raw_graphs.append(response.dict())
                
                with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
                    json.dump(all_raw_graphs, f, ensure_ascii=False)
                    
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                time.sleep(5)

        print("\nğŸ§¹ ë³‘í•© ë° ìµœì¢… ì €ì¥ ì¤‘...")
        final_graph = merge_graphs(all_raw_graphs)

        with open(FINAL_OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(final_graph, f, ensure_ascii=False, indent=2)

        print(f"ğŸ‰ ì™„ë£Œ! ë…¸ë“œ: {len(final_graph['nodes'])}, ê´€ê³„: {len(final_graph['relationships'])}")